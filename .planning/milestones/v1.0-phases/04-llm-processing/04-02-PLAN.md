---
phase: 04-llm-processing
plan: 02
type: execute
wave: 2
depends_on:
  - "04-01"
files_modified:
  - src/knowledge_hub/llm/processor.py
  - src/knowledge_hub/llm/__init__.py
  - tests/test_llm/__init__.py
  - tests/test_llm/test_schemas.py
  - tests/test_llm/test_prompts.py
  - tests/test_llm/test_processor.py
autonomous: true
requirements:
  - LLM-01
  - LLM-02
  - LLM-05
  - LLM-06
  - LLM-07
  - LLM-08
  - LLM-09
  - LLM-10

must_haves:
  truths:
    - "process_content(client, content) returns a fully populated NotionPage from any ExtractedContent"
    - "Gemini API calls are retried with exponential backoff + jitter on transient errors (429, 5xx), max 3 retries"
    - "Permanent API errors (400, 401, 403) are NOT retried"
    - "Partial/metadata-only extractions have priority overridden to Low regardless of LLM assignment"
    - "LLM response is validated via Pydantic schema -- ValidationError is caught and handled"
    - "All LLM module tests pass with mocked Gemini client (no real API calls)"
  artifacts:
    - path: "src/knowledge_hub/llm/processor.py"
      provides: "Main processing pipeline: ExtractedContent -> NotionPage via Gemini"
      contains: "async def process_content"
    - path: "src/knowledge_hub/llm/__init__.py"
      provides: "Public API re-exports"
      contains: "process_content"
    - path: "tests/test_llm/test_schemas.py"
      provides: "Schema validation tests for LLMResponse"
    - path: "tests/test_llm/test_prompts.py"
      provides: "Prompt template tests for build_system_prompt and build_user_content"
    - path: "tests/test_llm/test_processor.py"
      provides: "Processor tests with mocked Gemini client"
  key_links:
    - from: "src/knowledge_hub/llm/processor.py"
      to: "src/knowledge_hub/llm/client.py"
      via: "uses genai.Client for Gemini API calls"
      pattern: "genai\\.Client"
    - from: "src/knowledge_hub/llm/processor.py"
      to: "src/knowledge_hub/llm/schemas.py"
      via: "uses LLMResponse as response_schema"
      pattern: "response_schema=LLMResponse"
    - from: "src/knowledge_hub/llm/processor.py"
      to: "src/knowledge_hub/llm/prompts.py"
      via: "calls build_system_prompt and build_user_content"
      pattern: "build_system_prompt|build_user_content"
    - from: "src/knowledge_hub/llm/processor.py"
      to: "src/knowledge_hub/models/notion.py"
      via: "constructs NotionPage with KnowledgeEntry from LLMResponse + ExtractedContent"
      pattern: "NotionPage\\("
---

<objective>
Implement the LLM processor that wires schema, client, and prompts into a complete ExtractedContent -> NotionPage pipeline, plus comprehensive tests for all LLM modules.

Purpose: This is the core pipeline stage -- it takes extracted content from Phase 3 and produces structured knowledge entries ready for Notion in Phase 5. The processor handles Gemini API calls with retry logic, validates responses, maps LLM output to domain models, and applies post-processing rules (e.g., priority override for partial extractions).

Output: Working `process_content()` function and full test suite for all LLM modules.
</objective>

<execution_context>
@/Users/dbenger/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dbenger/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-llm-processing/04-RESEARCH.md
@.planning/phases/04-llm-processing/04-01-SUMMARY.md

@src/knowledge_hub/llm/schemas.py
@src/knowledge_hub/llm/client.py
@src/knowledge_hub/llm/prompts.py
@src/knowledge_hub/models/knowledge.py
@src/knowledge_hub/models/notion.py
@src/knowledge_hub/models/content.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement LLM processor with retry logic and response mapping</name>
  <files>src/knowledge_hub/llm/processor.py, src/knowledge_hub/llm/__init__.py</files>
  <action>
1. Create `src/knowledge_hub/llm/processor.py`:

**_is_retryable(error: BaseException) -> bool:**
- Return `True` for `ServerError` (all 5xx)
- Return `True` for `ClientError` with `error.code == 429` (rate limit)
- Return `False` for everything else (400, 401, 403 are permanent)
- Imports: `from google.genai.errors import ClientError, ServerError`

**_call_gemini(client, system_prompt, user_content) -> LLMResponse:** (async)
- Decorated with tenacity `@retry`:
  - `retry=retry_if_exception(_is_retryable)`
  - `wait=wait_exponential_jitter(initial=1, max=30, jitter=2)`
  - `stop=stop_after_attempt(4)` -- 1 initial + 3 retries = 4 total attempts (LLM-06)
  - `before_sleep=before_sleep_log(logger, logging.WARNING)`
  - `reraise=True`
- Calls `client.aio.models.generate_content()` with:
  - `model=GEMINI_MODEL` (imported from prompts)
  - `contents=user_content`
  - `config=types.GenerateContentConfig(system_instruction=system_prompt, response_mime_type="application/json", response_schema=LLMResponse, temperature=1.0)` -- temperature 1.0 per Gemini 3 recommendation
- Returns `response.parsed` (Pydantic-validated LLMResponse)
- Imports: `from google.genai import types`, `from knowledge_hub.llm.schemas import LLMResponse`, `from knowledge_hub.llm.prompts import GEMINI_MODEL`

**build_notion_page(llm_result: LLMResponse, content: ExtractedContent) -> NotionPage:**
- Combines LLM-generated fields with extraction-derived fields
- Creates `KnowledgeEntry` with:
  - `title=llm_result.title`
  - `category=llm_result.category`
  - `content_type=content.content_type` (from extraction, NOT LLM)
  - `source=content.url` (from extraction)
  - `author=content.author` (from extraction)
  - `date_added=datetime.now(timezone.utc)` (generated at processing time)
  - `status=Status.NEW` (always NEW)
  - `priority=llm_result.priority`
  - `tags=llm_result.tags`
  - `summary=llm_result.summary`
- Creates `KeyLearning` objects from `llm_result.key_learnings` (map LLMKeyLearning -> KeyLearning)
- Returns `NotionPage(entry=entry, summary_section=..., key_points=..., key_learnings=..., detailed_notes=...)`

**process_content(client: genai.Client, content: ExtractedContent) -> NotionPage:** (async, public API)
- Calls `build_system_prompt(content)` and `build_user_content(content)`
- Calls `_call_gemini(client, system_prompt, user_content)`
- Post-processing: if `content.extraction_status` in `(ExtractionStatus.PARTIAL, ExtractionStatus.METADATA_ONLY)`, override `llm_result.priority = Priority.LOW` regardless of LLM assignment (LLM-09)
- Calls `build_notion_page(llm_result, content)` and returns result
- Wraps `_call_gemini` in try/except for `pydantic.ValidationError` -- log the error and `response.text` for debugging, re-raise (per research pitfall 5 guidance). The caller (Phase 6 orchestrator) handles the error.
- Also handle `google.genai.errors.APIError` as the base exception for non-retryable API failures -- log and re-raise.

2. Update `src/knowledge_hub/llm/__init__.py`:
```python
"""LLM processing: structured content analysis via Gemini.

Public API:
    process_content(client, content) -> NotionPage
        Transforms ExtractedContent into a fully populated NotionPage
        via Gemini structured output with retry logic.
"""

from knowledge_hub.llm.client import get_gemini_client, reset_client
from knowledge_hub.llm.processor import process_content
from knowledge_hub.llm.schemas import LLMResponse

__all__ = [
    "get_gemini_client",
    "reset_client",
    "process_content",
    "LLMResponse",
]
```
  </action>
  <verify>
Run `python -c "from knowledge_hub.llm import process_content, get_gemini_client, LLMResponse; print('OK')"` to verify public API imports.
  </verify>
  <done>process_content async function wires prompts, Gemini client, and schema validation into a complete pipeline. Retry logic handles transient API errors with exponential backoff. Priority override applied for partial/metadata-only extractions. LLM module has clean public API via __init__.py.</done>
</task>

<task type="auto">
  <name>Task 2: Comprehensive test suite for all LLM modules</name>
  <files>tests/test_llm/__init__.py, tests/test_llm/test_schemas.py, tests/test_llm/test_prompts.py, tests/test_llm/test_processor.py</files>
  <action>
1. Create `tests/test_llm/__init__.py` (empty with docstring).

2. Create `tests/test_llm/test_schemas.py` -- Pure Pydantic validation tests (sync, no mocks):

**Test helper: `_make_valid_llm_response() -> dict`** -- returns a dict with all valid fields for LLMResponse construction. Use this as the base for variation tests.

Tests:
- `test_valid_llm_response_parses` -- full valid data parses to LLMResponse
- `test_llm_response_requires_title` -- missing title raises ValidationError
- `test_llm_response_category_enum` -- valid category string (e.g., "Engineering") maps to Category.ENGINEERING
- `test_llm_response_invalid_category` -- invalid category string raises ValidationError
- `test_llm_response_priority_enum` -- valid priority ("High") maps to Priority.HIGH
- `test_llm_response_tags_min_length` -- fewer than 3 tags raises ValidationError
- `test_llm_response_tags_max_length` -- more than 7 tags raises ValidationError
- `test_llm_response_key_points_min` -- fewer than 5 key points raises ValidationError
- `test_llm_response_key_points_max` -- more than 10 key points raises ValidationError
- `test_llm_response_key_learnings_min` -- fewer than 3 key learnings raises ValidationError
- `test_llm_key_learning_how_to_apply_nonempty` -- LLMKeyLearning with empty how_to_apply raises ValidationError
- `test_llm_key_learning_valid` -- valid LLMKeyLearning parses successfully

3. Create `tests/test_llm/test_prompts.py` -- Pure function tests (sync, no mocks):

**Test helper: `_make_content(**kwargs) -> ExtractedContent`** -- returns a valid ExtractedContent with sensible defaults, override via kwargs.

Tests:
- `test_build_system_prompt_contains_categories` -- output contains all 11 Category enum values
- `test_build_system_prompt_contains_seeded_tags` -- output contains representative seeded tags (spot-check 5-6)
- `test_build_system_prompt_contains_priority_criteria` -- output mentions "High", "Medium", "Low" with criteria
- `test_build_system_prompt_contains_key_learning_structure` -- output mentions "what", "why_it_matters", "how_to_apply" (LLM-07)
- `test_build_system_prompt_contains_importance_ordering` -- output mentions ordering by importance, not source order (LLM-08)
- `test_build_system_prompt_video_addendum` -- video content type triggers timestamp/duration instructions
- `test_build_system_prompt_short_content_addendum` -- content with word_count=200 triggers short content instructions
- `test_build_system_prompt_article_no_addendum` -- article with 2000 words gets base prompt only (no addendums)
- `test_build_user_content_article` -- article with title, author, text produces correct format with metadata + body
- `test_build_user_content_video` -- video with transcript uses transcript as body (not text)
- `test_build_user_content_fallback_description` -- content with only description falls back to description as body
- `test_build_user_content_omits_none_fields` -- None title/author not included in output

4. Create `tests/test_llm/test_processor.py` -- Async tests with mocked Gemini client:

**Test helper: `_make_mock_llm_response() -> LLMResponse`** -- returns a valid LLMResponse instance for processor tests.

**Test helper: `_make_content(**kwargs) -> ExtractedContent`** -- returns a valid ExtractedContent with sensible defaults.

Tests (all async, using `unittest.mock.AsyncMock` and `unittest.mock.patch`):
- `test_process_content_returns_notion_page` -- mock _call_gemini to return valid LLMResponse, verify NotionPage has correct entry fields (title, category, source, status=NEW, content_type from extraction)
- `test_process_content_maps_key_learnings` -- verify LLMKeyLearning objects are mapped to KeyLearning objects in NotionPage
- `test_process_content_partial_extraction_overrides_priority` -- content with ExtractionStatus.PARTIAL gets Priority.LOW regardless of LLM output
- `test_process_content_metadata_only_overrides_priority` -- content with ExtractionStatus.METADATA_ONLY gets Priority.LOW
- `test_process_content_full_extraction_preserves_priority` -- content with ExtractionStatus.FULL keeps LLM-assigned priority
- `test_process_content_uses_video_prompt` -- mock build_system_prompt, verify it's called with video content (confirm content-type routing)
- `test_build_notion_page_sets_status_new` -- verify entry.status is always Status.NEW
- `test_build_notion_page_uses_extraction_source` -- verify entry.source == content.url (not LLM-generated)
- `test_build_notion_page_uses_extraction_content_type` -- verify entry.content_type == content.content_type
- `test_is_retryable_server_error` -- ServerError returns True
- `test_is_retryable_rate_limit` -- ClientError(429) returns True
- `test_is_retryable_bad_request` -- ClientError(400) returns False
- `test_is_retryable_auth_error` -- ClientError(401) returns False

Testing approach:
- Use `unittest.mock.patch` on `knowledge_hub.llm.processor._call_gemini` to avoid any real Gemini API calls
- For _is_retryable tests, construct mock error objects with the right type and attributes
- All processor tests are async (pytest-asyncio with `asyncio_mode = "auto"` from pyproject.toml)
- Follow the existing pattern from Phase 2/3: `_make_*()` helper factories with override kwargs
  </action>
  <verify>
Run `uv run pytest tests/test_llm/ -v` -- all tests should pass. Then run `uv run pytest tests/ -q` to confirm no regressions in existing test suites.
  </verify>
  <done>12+ schema validation tests confirm LLMResponse enforces all Field constraints. 12+ prompt tests confirm system prompt contains all required instructions and content-type routing works. 13+ processor tests confirm pipeline wiring, priority override, status/source/content_type mapping, and retry classification. Full test suite passes with zero regressions.</done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_llm/ -v` -- all LLM tests pass
2. `uv run pytest tests/ -q` -- full test suite passes (no regressions)
3. `python -c "from knowledge_hub.llm import process_content, get_gemini_client, LLMResponse; print('Public API OK')"` -- clean imports
4. Spot-check: `_is_retryable` correctly classifies ServerError as retryable and ClientError(400) as non-retryable
5. Spot-check: process_content with ExtractionStatus.PARTIAL produces Priority.LOW
</verification>

<success_criteria>
- process_content(client, content) is callable and returns NotionPage (LLM-01, LLM-02)
- Gemini calls use structured output with LLMResponse schema (LLM-05)
- Retry logic: exponential backoff + jitter, max 4 attempts, retries 429/5xx only (LLM-06)
- Priority override for partial/metadata-only extractions (LLM-09)
- Content-type-specific prompt routing verified in tests (LLM-10)
- Key learning structure (What/Why/How) mapped correctly (LLM-07)
- Importance-ordering instruction present in prompts (LLM-08)
- All LLM tests pass with mocked Gemini client
- Existing 69+ tests unaffected
</success_criteria>

<output>
After completion, create `.planning/phases/04-llm-processing/04-02-SUMMARY.md`
</output>
