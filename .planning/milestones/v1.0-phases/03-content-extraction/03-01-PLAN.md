---
phase: 03-content-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/knowledge_hub/models/content.py
  - src/knowledge_hub/models/__init__.py
  - tests/test_models/test_content.py
  - src/knowledge_hub/extraction/router.py
  - src/knowledge_hub/extraction/paywall.py
  - src/knowledge_hub/config/paywalled_domains.yaml
  - src/knowledge_hub/extraction/article.py
  - src/knowledge_hub/extraction/youtube.py
  - src/knowledge_hub/extraction/pdf.py
  - pyproject.toml
autonomous: true
requirements:
  - EXTRACT-01
  - EXTRACT-02
  - EXTRACT-03
  - EXTRACT-04
  - EXTRACT-06
  - EXTRACT-07
  - EXTRACT-08

must_haves:
  truths:
    - "ExtractionStatus enum has four values: full, partial, metadata_only, failed"
    - "ExtractedContent model uses extraction_status field instead of is_partial"
    - "URL pattern matching correctly routes YouTube, PDF, Substack, and Medium URLs to their content types"
    - "Unknown URLs default to ARTICLE content type"
    - "Paywalled domain list is loaded from a YAML config file, not hardcoded"
    - "Article extractor uses trafilatura bare_extraction() wrapped in asyncio.to_thread()"
    - "YouTube extractor uses YouTubeTranscriptApi instance fetch() method (not deprecated static methods)"
    - "YouTube extractor catches TranscriptsDisabled/NoTranscriptFound and returns metadata_only status"
    - "PDF extractor checks Content-Length before download and enforces 20MB cap"
    - "All sync library calls are wrapped in asyncio.to_thread() to avoid blocking the event loop"
  artifacts:
    - path: "src/knowledge_hub/models/content.py"
      provides: "ExtractionStatus enum and updated ExtractedContent model"
      contains: "class ExtractionStatus"
    - path: "src/knowledge_hub/extraction/router.py"
      provides: "URL pattern matching and content type detection"
      exports: ["detect_content_type"]
    - path: "src/knowledge_hub/extraction/paywall.py"
      provides: "Paywalled domain checking"
      exports: ["is_paywalled_domain", "load_paywalled_domains"]
    - path: "src/knowledge_hub/config/paywalled_domains.yaml"
      provides: "Configurable paywalled domain list"
      contains: "domains:"
    - path: "src/knowledge_hub/extraction/article.py"
      provides: "trafilatura-based article extraction"
      exports: ["extract_article"]
    - path: "src/knowledge_hub/extraction/youtube.py"
      provides: "YouTube transcript extraction with fallback"
      exports: ["extract_youtube", "extract_video_id"]
    - path: "src/knowledge_hub/extraction/pdf.py"
      provides: "PDF download and text extraction"
      exports: ["extract_pdf"]
  key_links:
    - from: "src/knowledge_hub/extraction/router.py"
      to: "src/knowledge_hub/models/content.py"
      via: "imports ContentType enum"
      pattern: "from knowledge_hub\\.models\\.content import ContentType"
    - from: "src/knowledge_hub/extraction/article.py"
      to: "trafilatura"
      via: "bare_extraction() for body text and metadata"
      pattern: "bare_extraction"
    - from: "src/knowledge_hub/extraction/youtube.py"
      to: "youtube_transcript_api"
      via: "YouTubeTranscriptApi().fetch() instance method"
      pattern: "ytt_api\\.fetch"
    - from: "src/knowledge_hub/extraction/pdf.py"
      to: "pypdf"
      via: "PdfReader(BytesIO()) for text extraction"
      pattern: "PdfReader"
---

<objective>
Create all extraction building blocks: update the data model with ExtractionStatus enum, implement content type routing by URL patterns, configure paywalled domain detection, and build three type-specific extractors (article, YouTube, PDF).

Purpose: These are the individual components that the extraction pipeline (Plan 02) will orchestrate. Each extractor is self-contained and handles its own failure modes.
Output: Updated model, 3 extractors, URL router, paywall config, new dependencies installed.
</objective>

<execution_context>
@/Users/dbenger/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dbenger/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-content-extraction/03-CONTEXT.md
@.planning/phases/03-content-extraction/03-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Model update + content type router + paywall config + dependencies</name>
  <files>
    src/knowledge_hub/models/content.py
    src/knowledge_hub/models/__init__.py
    tests/test_models/test_content.py
    src/knowledge_hub/extraction/router.py
    src/knowledge_hub/extraction/paywall.py
    src/knowledge_hub/config/paywalled_domains.yaml
    pyproject.toml
  </files>
  <action>
    **Step 1: Add new dependencies.**
    Run `uv add trafilatura youtube-transcript-api pypdf` to add the three extraction libraries. Verify they appear in pyproject.toml dependencies.

    **Step 2: Update content model.**
    In `src/knowledge_hub/models/content.py`:
    - Add `ExtractionStatus(str, Enum)` with values: `FULL = "full"`, `PARTIAL = "partial"`, `METADATA_ONLY = "metadata_only"`, `FAILED = "failed"`.
    - Replace `is_partial: bool = False` field on `ExtractedContent` with `extraction_status: ExtractionStatus = ExtractionStatus.FULL`.
    - Keep all other existing fields unchanged.

    In `src/knowledge_hub/models/__init__.py`:
    - Add `ExtractionStatus` to the imports from `content.py` and to `__all__`.

    In `tests/test_models/test_content.py`:
    - Update any test referencing `is_partial` to use `extraction_status` instead.
    - Add a test verifying ExtractionStatus enum has exactly 4 values.
    - Add a test verifying ExtractedContent defaults to `ExtractionStatus.FULL`.

    Run `uv run pytest tests/test_models/test_content.py -v` to confirm tests pass.

    **Step 3: Create content type router.**
    Create `src/knowledge_hub/extraction/router.py`:
    - Define compiled regex patterns (ordered by specificity):
      - `YOUTUBE_PATTERN`: matches `youtube.com/watch?...v=`, `youtu.be/`, `youtube.com/shorts/`, `youtube.com/embed/`
      - `PDF_PATTERN`: matches `.pdf` extension (case-insensitive, with optional query params)
      - `SUBSTACK_PATTERN`: matches `*.substack.com/`
      - `MEDIUM_PATTERN`: matches `medium.com/` or `*.medium.com/`
    - Implement `detect_content_type(url: str) -> ContentType` that checks patterns in order and returns `ContentType.VIDEO`, `ContentType.PDF`, `ContentType.NEWSLETTER`, or `ContentType.ARTICLE` (default fallback).
    - Import `ContentType` from `knowledge_hub.models.content`.

    **Step 4: Create paywall config.**
    Create `src/knowledge_hub/config/` directory with `__init__.py`.
    Create `src/knowledge_hub/config/paywalled_domains.yaml` with a `domains:` list including: nytimes.com, washingtonpost.com, wsj.com, ft.com, economist.com, thetimes.co.uk, bloomberg.com, theathletic.com, hbr.org, businessinsider.com, seekingalpha.com, medium.com.

    Create `src/knowledge_hub/extraction/paywall.py`:
    - `load_paywalled_domains() -> set[str]`: Load domains from YAML config file. Use `importlib.resources` or `pathlib.Path` relative to package to locate the YAML file. Cache result with `@functools.lru_cache`.
    - `is_paywalled_domain(url: str) -> bool`: Parse URL hostname, check against loaded domain set. Handle subdomains (e.g., `www.nytimes.com` matches `nytimes.com`).
    - Use PyYAML (comes with trafilatura) or simple line parsing. If PyYAML is not directly available, use a plain text file with one domain per line instead of YAML.
  </action>
  <verify>
    - `uv run pytest tests/test_models/test_content.py -v` passes (model update)
    - `uv run python -c "from knowledge_hub.extraction.router import detect_content_type; from knowledge_hub.models import ContentType; assert detect_content_type('https://youtube.com/watch?v=abc123') == ContentType.VIDEO; assert detect_content_type('https://example.com/doc.pdf') == ContentType.PDF; assert detect_content_type('https://example.substack.com/p/hello') == ContentType.NEWSLETTER; assert detect_content_type('https://example.com/article') == ContentType.ARTICLE; print('Router OK')"` prints "Router OK"
    - `uv run python -c "from knowledge_hub.extraction.paywall import is_paywalled_domain; assert is_paywalled_domain('https://www.nytimes.com/article'); assert not is_paywalled_domain('https://example.com/page'); print('Paywall OK')"` prints "Paywall OK"
  </verify>
  <done>
    ExtractionStatus enum exists with 4 values. ExtractedContent uses extraction_status instead of is_partial. All existing model tests pass. Content type router correctly classifies YouTube, PDF, Substack, Medium, and default article URLs. Paywalled domain list loads from config file and domain checking works with subdomain handling. trafilatura, youtube-transcript-api, and pypdf are installed as dependencies.
  </done>
</task>

<task type="auto">
  <name>Task 2: Article, YouTube, and PDF extractors</name>
  <files>
    src/knowledge_hub/extraction/article.py
    src/knowledge_hub/extraction/youtube.py
    src/knowledge_hub/extraction/pdf.py
  </files>
  <action>
    **Step 1: Create article extractor.**
    Create `src/knowledge_hub/extraction/article.py`:
    - `async def extract_article(url: str) -> ExtractedContent`: Main entry point for article extraction.
    - Uses `trafilatura.fetch_url(url)` wrapped in `asyncio.to_thread()` to download.
    - Uses `trafilatura.bare_extraction(downloaded, url=url)` wrapped in `asyncio.to_thread()` to extract.
    - If `fetch_url` returns `None`, return ExtractedContent with `extraction_status=FAILED`.
    - If `bare_extraction` returns `None`, return ExtractedContent with `extraction_status=METADATA_ONLY`.
    - Map trafilatura Document fields to ExtractedContent fields:
      - `doc.text` -> `text`
      - `doc.title` -> `title`
      - `doc.author` -> `author`
      - `doc.date` -> `published_date`
      - `doc.sitename` or `doc.hostname` -> `source_domain`
      - `doc.description` -> `description`
      - Calculate `word_count` from `text` using `len(text.split())` if text exists.
    - Set `extraction_method = "trafilatura"`.
    - Set `content_type = ContentType.ARTICLE`.
    - Set `extraction_status = FULL` if text is present, `METADATA_ONLY` if only metadata fields populated.
    - Check `is_paywalled_domain(url)` -- if paywalled and text is short/empty, set `extraction_status = PARTIAL`.

    **Step 2: Create YouTube extractor.**
    Create `src/knowledge_hub/extraction/youtube.py`:
    - `extract_video_id(url: str) -> str | None`: Extract 11-character video ID using regex matching `youtube.com/watch?...v=`, `youtu.be/`, `youtube.com/shorts/`, `youtube.com/embed/`. Handle URLs with additional query params.
    - `async def extract_youtube(url: str) -> ExtractedContent`: Main entry point.
    - Create `YouTubeTranscriptApi()` instance. NEVER use deprecated static methods (`get_transcript`, etc.).
    - Call `ytt_api.fetch(video_id, languages=["en"])` wrapped in `asyncio.to_thread()`.
    - Join transcript snippets: `" ".join(snippet.text for snippet in transcript)`.
    - Map to ExtractedContent:
      - `transcript` -> joined text
      - `content_type` -> `ContentType.VIDEO`
      - `source_domain` -> `"youtube.com"`
      - `extraction_method` -> `"youtube-transcript-api"`
      - `extraction_status` -> `FULL`
    - Catch `TranscriptsDisabled`, `NoTranscriptFound` -> return with `extraction_status=METADATA_ONLY`, `transcript=None`.
    - Catch `VideoUnavailable`, `InvalidVideoId` -> return with `extraction_status=FAILED`.
    - If `extract_video_id` returns `None`, return with `extraction_status=FAILED`.
    - Import exceptions from `youtube_transcript_api._errors` (TranscriptsDisabled, NoTranscriptFound, VideoUnavailable, InvalidVideoId).

    **Step 3: Create PDF extractor.**
    Create `src/knowledge_hub/extraction/pdf.py`:
    - Define `MAX_PDF_SIZE_BYTES = 20 * 1024 * 1024` (20MB).
    - `async def extract_pdf(url: str) -> ExtractedContent`: Main entry point.
    - Use `httpx.AsyncClient(timeout=httpx.Timeout(25.0), follow_redirects=True)`.
    - First attempt HEAD request to check `Content-Length`. If > 20MB, return `extraction_status=METADATA_ONLY` with note about size.
    - If HEAD fails or no Content-Length, proceed with GET.
    - After GET, check `len(response.content) > MAX_PDF_SIZE_BYTES`.
    - Use `PdfReader(BytesIO(response.content))` wrapped in `asyncio.to_thread()`.
    - Extract text from all pages: `page.extract_text()` in `asyncio.to_thread()` for each page.
    - Extract metadata: `reader.metadata.title`, `reader.metadata.author` if available.
    - Map to ExtractedContent:
      - `text` -> joined page text
      - `title` -> from PDF metadata
      - `author` -> from PDF metadata
      - `content_type` -> `ContentType.PDF`
      - `source_domain` -> parsed from URL hostname
      - `extraction_method` -> `"pypdf"`
      - `extraction_status` -> `FULL` if text extracted, `METADATA_ONLY` if no text (scanned/image PDF)
      - `word_count` -> from extracted text
    - Catch `httpx.HTTPError` for download failures -> return `extraction_status=FAILED`.
    - Catch pypdf exceptions -> return `extraction_status=FAILED`.
  </action>
  <verify>
    - `uv run python -c "from knowledge_hub.extraction.article import extract_article; print('article import OK')"` succeeds
    - `uv run python -c "from knowledge_hub.extraction.youtube import extract_youtube, extract_video_id; assert extract_video_id('https://youtube.com/watch?v=dQw4w9WgXcQ') == 'dQw4w9WgXcQ'; assert extract_video_id('https://youtu.be/dQw4w9WgXcQ') == 'dQw4w9WgXcQ'; assert extract_video_id('https://youtube.com/shorts/dQw4w9WgXcQ') == 'dQw4w9WgXcQ'; assert extract_video_id('not-a-url') is None; print('youtube import + video_id OK')"` succeeds
    - `uv run python -c "from knowledge_hub.extraction.pdf import extract_pdf; print('pdf import OK')"` succeeds
    - `uv run pytest tests/ -v` -- all existing tests still pass
  </verify>
  <done>
    Three type-specific extractors exist and are importable. Article extractor wraps trafilatura with async, handles None results, maps Document fields to ExtractedContent. YouTube extractor uses instance fetch() method, catches all 4 exception types, falls back to metadata_only for disabled/missing captions. PDF extractor downloads with size cap, extracts text from all pages, reads document metadata. All extractors return ExtractedContent instances with appropriate ExtractionStatus values. All sync library calls use asyncio.to_thread().
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -v` -- all existing tests pass (no regressions from model change)
2. All three extractors importable without errors
3. Content type router classifies YouTube, PDF, Substack, Medium, and generic URLs correctly
4. Paywall domain checker loads from YAML config and handles subdomains
5. ExtractionStatus enum has exactly 4 values and ExtractedContent defaults to FULL
</verification>

<success_criteria>
- ExtractionStatus enum replaces is_partial bool across the codebase
- detect_content_type() routes URLs to correct ContentType values
- Three async extractors (article, youtube, pdf) handle success and failure paths
- All sync library calls wrapped in asyncio.to_thread()
- Paywalled domain list loaded from config file (not hardcoded)
- All existing tests pass without modification (except is_partial -> extraction_status updates)
</success_criteria>

<output>
After completion, create `.planning/phases/03-content-extraction/03-01-SUMMARY.md`
</output>
