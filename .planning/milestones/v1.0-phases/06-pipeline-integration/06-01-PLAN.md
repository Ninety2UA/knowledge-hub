---
phase: 06-pipeline-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/knowledge_hub/slack/client.py
  - src/knowledge_hub/slack/notifier.py
  - src/knowledge_hub/slack/handlers.py
  - src/knowledge_hub/slack/__init__.py
  - src/knowledge_hub/models/content.py
  - src/knowledge_hub/llm/prompts.py
autonomous: true
requirements: [NOTIFY-01, NOTIFY-02, NOTIFY-03, NOTIFY-04]

must_haves:
  truths:
    - "A URL pasted in Slack triggers extract -> LLM -> Notion -> Slack reply with Notion link"
    - "A failed extraction produces a Slack thread reply describing the failure stage and error"
    - "A duplicate URL produces a Slack thread reply linking to the existing Notion page"
    - "A checkmark reaction is added to the Slack message when all URLs succeed"
    - "An X reaction is added to the Slack message when any URL fails"
    - "Notification failures never crash the pipeline or prevent other URLs from processing"
    - "User notes from Slack messages are forwarded into the LLM prompt"
  artifacts:
    - path: "src/knowledge_hub/slack/client.py"
      provides: "AsyncWebClient singleton for Slack API"
      exports: ["get_slack_client", "reset_client"]
    - path: "src/knowledge_hub/slack/notifier.py"
      provides: "Notification functions for success, error, duplicate, and reactions"
      exports: ["notify_success", "notify_error", "notify_duplicate", "add_reaction"]
    - path: "src/knowledge_hub/slack/handlers.py"
      provides: "Full pipeline orchestration in process_message_urls"
      contains: "process_content"
    - path: "src/knowledge_hub/models/content.py"
      provides: "user_note field on ExtractedContent"
      contains: "user_note"
    - path: "src/knowledge_hub/llm/prompts.py"
      provides: "user_note inclusion in LLM user content"
      contains: "user_note"
  key_links:
    - from: "src/knowledge_hub/slack/handlers.py"
      to: "knowledge_hub.extraction.extract_content"
      via: "await extract_content(url)"
      pattern: "extract_content"
    - from: "src/knowledge_hub/slack/handlers.py"
      to: "knowledge_hub.llm.process_content"
      via: "await process_content(client, content)"
      pattern: "process_content"
    - from: "src/knowledge_hub/slack/handlers.py"
      to: "knowledge_hub.notion.create_notion_page"
      via: "await create_notion_page(notion_page)"
      pattern: "create_notion_page"
    - from: "src/knowledge_hub/slack/handlers.py"
      to: "knowledge_hub.slack.notifier"
      via: "await notify_success/notify_error/notify_duplicate/add_reaction"
      pattern: "notify_"
    - from: "src/knowledge_hub/slack/notifier.py"
      to: "knowledge_hub.slack.client.get_slack_client"
      via: "await get_slack_client()"
      pattern: "get_slack_client"
---

<objective>
Wire all pipeline stages (extraction, LLM, Notion) into process_message_urls and add Slack notifications for every outcome.

Purpose: Close the integration gap identified in the milestone audit -- currently process_message_urls calls extract_content but discards the result, never calling process_content or create_notion_page. Also close the user_note data-loss gap where notes extracted from Slack are silently dropped before LLM processing.

Output: Fully wired pipeline producing Notion pages and Slack thread replies for success, failure, and duplicate outcomes, with per-message emoji reactions.
</objective>

<execution_context>
@/Users/dbenger/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dbenger/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-pipeline-integration/06-RESEARCH.md
@.planning/phases/05-notion-output/05-02-SUMMARY.md
@src/knowledge_hub/slack/handlers.py
@src/knowledge_hub/slack/__init__.py
@src/knowledge_hub/models/content.py
@src/knowledge_hub/llm/prompts.py
@src/knowledge_hub/llm/processor.py
@src/knowledge_hub/notion/service.py
@src/knowledge_hub/notion/models.py
@src/knowledge_hub/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Slack client singleton and notifier module</name>
  <files>
    src/knowledge_hub/slack/client.py
    src/knowledge_hub/slack/notifier.py
    src/knowledge_hub/models/content.py
    src/knowledge_hub/llm/prompts.py
  </files>
  <action>
**1. Create `src/knowledge_hub/slack/client.py`** -- AsyncWebClient singleton following the established pattern from `notion/client.py` and `llm/client.py`:
- Module-level `_client: AsyncWebClient | None = None`
- `async def get_slack_client() -> AsyncWebClient`: lazy init from `get_settings().slack_bot_token`
- `def reset_client() -> None`: sets `_client = None` (for testing)
- Import `AsyncWebClient` from `slack_sdk.web.async_client`
- Import `get_settings` from `knowledge_hub.config`

**2. Create `src/knowledge_hub/slack/notifier.py`** with these functions:

```python
async def notify_success(channel_id: str, timestamp: str, result: PageResult) -> None:
```
- Thread reply via `chat_postMessage(channel=channel_id, thread_ts=timestamp, text=...)`
- Message format: `"Saved to Notion: <{result.page_url}|{result.title}>"`
- Wrap in try/except `SlackApiError` -- log warning, never raise

```python
async def notify_error(channel_id: str, timestamp: str, url: str, stage: str, detail: str) -> None:
```
- Thread reply with stage-specific error
- Message format: `"Failed to process <{url}>: {stage} â€” {detail}"`
- Same fire-and-forget pattern

```python
async def notify_duplicate(channel_id: str, timestamp: str, url: str, duplicate: DuplicateResult) -> None:
```
- Thread reply linking existing page
- Message format: `"Already saved: <{duplicate.page_url}|{duplicate.title}>"`
- Same fire-and-forget pattern

```python
async def add_reaction(channel_id: str, timestamp: str, emoji: str) -> None:
```
- `reactions_add(channel=channel_id, name=emoji, timestamp=timestamp)`
- Catch `SlackApiError` and handle `missing_scope`, `already_reacted`, `no_item_specified` gracefully (log warning, do not raise)
- All other `SlackApiError` codes: log error, do not raise

All four functions import `get_slack_client` from `knowledge_hub.slack.client` and `SlackApiError` from `slack_sdk.errors`. Add module-level `logger = logging.getLogger(__name__)`.

**3. Add `user_note` field to `ExtractedContent`** in `src/knowledge_hub/models/content.py`:
- Add `user_note: str | None = None` field after `extraction_status`
- This closes the INGEST-03 data-loss gap

**4. Update `build_user_content()` in `src/knowledge_hub/llm/prompts.py`**:
- After the author/source metadata block and before the body divider, add:
  ```python
  if content.user_note:
      parts.append(f"User Note: {content.user_note}")
  ```
- This ensures user notes from Slack reach the LLM prompt
  </action>
  <verify>
- `python -c "from knowledge_hub.slack.client import get_slack_client, reset_client"` succeeds
- `python -c "from knowledge_hub.slack.notifier import notify_success, notify_error, notify_duplicate, add_reaction"` succeeds
- `python -c "from knowledge_hub.models.content import ExtractedContent; e = ExtractedContent(url='x', content_type='Article', user_note='test'); assert e.user_note == 'test'"` succeeds
- `python -c "from knowledge_hub.llm.prompts import build_user_content; from knowledge_hub.models.content import ExtractedContent, ContentType; c = ExtractedContent(url='x', content_type=ContentType.ARTICLE, text='body', user_note='my note'); r = build_user_content(c); assert 'User Note: my note' in r"` succeeds
- `ruff check src/knowledge_hub/slack/client.py src/knowledge_hub/slack/notifier.py src/knowledge_hub/models/content.py src/knowledge_hub/llm/prompts.py` passes
  </verify>
  <done>
Slack client singleton exists with same lazy-init pattern as other clients. Notifier module has 4 fire-and-forget functions (success, error, duplicate, reaction) that never crash the pipeline. ExtractedContent has user_note field. build_user_content includes user notes in LLM prompt.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire full pipeline in process_message_urls and update exports</name>
  <files>
    src/knowledge_hub/slack/handlers.py
    src/knowledge_hub/slack/__init__.py
  </files>
  <action>
**1. Rewrite `process_message_urls()` in `src/knowledge_hub/slack/handlers.py`** to orchestrate the full pipeline:

Add imports at top:
```python
from knowledge_hub.llm import get_gemini_client, process_content
from knowledge_hub.models.content import ExtractionStatus
from knowledge_hub.notion import create_notion_page
from knowledge_hub.notion.models import DuplicateResult
from knowledge_hub.slack.notifier import add_reaction, notify_duplicate, notify_error, notify_success
```

Remove the existing `from knowledge_hub.extraction import extract_content` import at line 9 and move it into the new imports block (keep it, just consolidate).

Remove `from knowledge_hub.models.slack import SlackEvent` since SlackEvent is no longer created in the pipeline.

Rewrite `process_message_urls` body:

```python
async def process_message_urls(
    channel_id: str,
    timestamp: str,
    user_id: str,
    text: str,
    urls: list[str],
    user_note: str | None,
) -> None:
    """Resolve URLs and process each through the full pipeline.

    For each URL: extract content -> LLM analysis -> Notion page creation -> Slack notification.
    Each URL is processed independently -- one failure does not abort others.
    A single emoji reaction is added to the original message after all URLs are processed.
    """
    resolved = await resolve_urls(urls)

    logger.info(
        "Resolved %d/%d URLs for message %s",
        len(resolved),
        len(urls),
        timestamp,
    )

    gemini_client = get_gemini_client()
    all_succeeded = True

    for url in resolved:
        try:
            # Stage 1: Extract content
            content = await extract_content(url)
            if content.extraction_status == ExtractionStatus.FAILED:
                await notify_error(channel_id, timestamp, url, "extraction",
                                   "Content could not be extracted")
                all_succeeded = False
                continue

            # Pass user_note through to content for LLM prompt
            content.user_note = user_note

            # Stage 2: LLM processing
            notion_page = await process_content(gemini_client, content)

            # Stage 3: Notion page creation
            result = await create_notion_page(notion_page)

            if isinstance(result, DuplicateResult):
                await notify_duplicate(channel_id, timestamp, url, result)
                continue  # Duplicate is not a failure

            # Success
            await notify_success(channel_id, timestamp, result)
            logger.info("Pipeline complete for %s -> %s", url, result.page_url)

        except Exception as exc:
            logger.error("Pipeline failed for %s: %s", url, exc, exc_info=True)
            stage = _classify_stage(exc)
            await notify_error(channel_id, timestamp, url, stage, str(exc))
            all_succeeded = False

    # One reaction per message (not per URL) -- checkmark if all succeeded, X if any failed
    emoji = "white_check_mark" if all_succeeded else "x"
    await add_reaction(channel_id, timestamp, emoji)
```

Add helper function `_classify_stage()` in the same file:

```python
def _classify_stage(exc: Exception) -> str:
    """Classify which pipeline stage an exception originated from.

    Uses exception module path to identify the stage. Falls back to 'processing'
    for unclassifiable exceptions.
    """
    module = type(exc).__module__ or ""
    if "extraction" in module:
        return "extraction"
    if "llm" in module or "genai" in module or "google" in module:
        return "llm"
    if "notion" in module:
        return "notion"
    return "processing"
```

**2. Update `src/knowledge_hub/slack/__init__.py`** to export the new public API:

```python
"""Slack ingress: webhook handling, signature verification, URL extraction, and notifications."""

from knowledge_hub.slack.client import get_slack_client, reset_client
from knowledge_hub.slack.notifier import add_reaction, notify_duplicate, notify_error, notify_success
from knowledge_hub.slack.router import router

__all__ = [
    "add_reaction",
    "get_slack_client",
    "notify_duplicate",
    "notify_error",
    "notify_success",
    "reset_client",
    "router",
]
```
  </action>
  <verify>
- `python -c "from knowledge_hub.slack import router, get_slack_client, notify_success, notify_error, notify_duplicate, add_reaction"` succeeds
- `python -c "from knowledge_hub.slack.handlers import process_message_urls, _classify_stage"` succeeds
- `ruff check src/knowledge_hub/slack/` passes
- `pytest tests/ -x --timeout=30` passes (all 183 existing tests should still pass -- the pipeline changes only affect the background task which is mocked in existing tests)
  </verify>
  <done>
process_message_urls wires extract -> LLM -> Notion -> notify for each URL. Failed extractions skip LLM/Notion and notify error. Duplicates notify with existing page link. Unhandled exceptions are caught with stage classification and error notification. Single emoji reaction per message after all URLs processed. Slack package exports updated with client and notifier public API.
  </done>
</task>

</tasks>

<verification>
1. All imports succeed: `python -c "from knowledge_hub.slack.client import get_slack_client; from knowledge_hub.slack.notifier import notify_success, notify_error, notify_duplicate, add_reaction; from knowledge_hub.slack.handlers import process_message_urls"`
2. ExtractedContent accepts user_note: `python -c "from knowledge_hub.models.content import ExtractedContent; e = ExtractedContent(url='x', content_type='Article', user_note='note'); assert e.user_note == 'note'"`
3. build_user_content includes user_note: `python -c "from knowledge_hub.llm.prompts import build_user_content; from knowledge_hub.models.content import ExtractedContent, ContentType; c = ExtractedContent(url='x', content_type=ContentType.ARTICLE, text='body', user_note='my note'); assert 'User Note: my note' in build_user_content(c)"`
4. Existing test suite passes: `pytest tests/ -x`
5. Lint clean: `ruff check src/knowledge_hub/`
</verification>

<success_criteria>
- Slack client singleton module exists following established pattern
- Notifier module has 4 fire-and-forget functions with graceful error handling
- process_message_urls chains extract -> LLM -> Notion -> notify per URL
- Failed extractions produce error thread replies (NOTIFY-02)
- Duplicate URLs produce thread replies with existing page link (NOTIFY-03)
- Successful URLs produce thread replies with new Notion page link (NOTIFY-01)
- Single emoji reaction per message: checkmark if all succeed, X if any fail (NOTIFY-04)
- user_note flows from Slack through ExtractedContent into LLM prompt
- All notification failures are caught and logged (never crash pipeline)
- 183 existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-pipeline-integration/06-01-SUMMARY.md`
</output>
