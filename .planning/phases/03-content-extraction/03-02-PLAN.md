---
phase: 03-content-extraction
plan: 02
type: tdd
wave: 2
depends_on:
  - "03-01"
files_modified:
  - src/knowledge_hub/extraction/timeout.py
  - src/knowledge_hub/extraction/__init__.py
  - src/knowledge_hub/slack/handlers.py
  - tests/test_extraction/__init__.py
  - tests/test_extraction/test_router.py
  - tests/test_extraction/test_article.py
  - tests/test_extraction/test_youtube.py
  - tests/test_extraction/test_pdf.py
  - tests/test_extraction/test_paywall.py
  - tests/test_extraction/test_pipeline.py
autonomous: true
requirements:
  - EXTRACT-01
  - EXTRACT-02
  - EXTRACT-03
  - EXTRACT-04
  - EXTRACT-05
  - EXTRACT-06
  - EXTRACT-07
  - EXTRACT-08

must_haves:
  truths:
    - "Extraction pipeline completes or times out within 30 seconds per URL"
    - "Transient failures get one retry within the 30-second budget"
    - "Timeout returns ExtractedContent with FAILED status, not an exception"
    - "extract_content() is the single public API that routes, extracts, and handles timeouts"
    - "Content type router tests cover YouTube, PDF, Substack, Medium, and default article URLs"
    - "Article extractor tests mock trafilatura and verify ExtractedContent field mapping"
    - "YouTube extractor tests mock youtube-transcript-api and verify fallback on TranscriptsDisabled"
    - "PDF extractor tests mock httpx and pypdf and verify 20MB size cap enforcement"
    - "Paywall tests verify domain loading from config and subdomain matching"
    - "Pipeline test verifies routing -> extraction -> timeout flow end-to-end with mocks"
  artifacts:
    - path: "src/knowledge_hub/extraction/timeout.py"
      provides: "30-second wall-clock timeout wrapper with retry logic"
      exports: ["extract_with_timeout"]
    - path: "src/knowledge_hub/extraction/__init__.py"
      provides: "Public API re-export for extraction package"
      exports: ["extract_content"]
    - path: "tests/test_extraction/test_router.py"
      provides: "Content type detection tests"
      min_lines: 30
    - path: "tests/test_extraction/test_article.py"
      provides: "Article extractor tests with mocked trafilatura"
      min_lines: 40
    - path: "tests/test_extraction/test_youtube.py"
      provides: "YouTube extractor tests with mocked youtube-transcript-api"
      min_lines: 50
    - path: "tests/test_extraction/test_pdf.py"
      provides: "PDF extractor tests with mocked httpx and pypdf"
      min_lines: 40
    - path: "tests/test_extraction/test_paywall.py"
      provides: "Paywall domain detection tests"
      min_lines: 20
    - path: "tests/test_extraction/test_pipeline.py"
      provides: "Pipeline integration tests with mocked extractors"
      min_lines: 40
  key_links:
    - from: "src/knowledge_hub/extraction/__init__.py"
      to: "src/knowledge_hub/extraction/timeout.py"
      via: "re-exports extract_content which calls extract_with_timeout"
      pattern: "extract_with_timeout"
    - from: "src/knowledge_hub/extraction/timeout.py"
      to: "src/knowledge_hub/extraction/router.py"
      via: "calls detect_content_type to route URL"
      pattern: "detect_content_type"
    - from: "src/knowledge_hub/extraction/timeout.py"
      to: "src/knowledge_hub/extraction/article.py"
      via: "dispatches to extract_article for ARTICLE type"
      pattern: "extract_article"
    - from: "src/knowledge_hub/extraction/timeout.py"
      to: "src/knowledge_hub/extraction/youtube.py"
      via: "dispatches to extract_youtube for VIDEO type"
      pattern: "extract_youtube"
    - from: "src/knowledge_hub/extraction/timeout.py"
      to: "src/knowledge_hub/extraction/pdf.py"
      via: "dispatches to extract_pdf for PDF type"
      pattern: "extract_pdf"
---

<objective>
Wire all extraction components into a timeout-guarded pipeline with a single public API, then write comprehensive TDD tests for every extraction component.

Purpose: The pipeline orchestration connects routing, extraction, paywall detection, retry, and timeout into a single `extract_content(url)` call. Tests verify each component in isolation with mocked external dependencies, plus integration tests for the full pipeline flow.
Output: Working extraction pipeline with 30s timeout, comprehensive test suite.
</objective>

<execution_context>
@/Users/dbenger/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dbenger/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-content-extraction/03-CONTEXT.md
@.planning/phases/03-content-extraction/03-RESEARCH.md
@.planning/phases/03-content-extraction/03-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Timeout wrapper + extraction pipeline + public API</name>
  <files>
    src/knowledge_hub/extraction/timeout.py
    src/knowledge_hub/extraction/__init__.py
    src/knowledge_hub/slack/handlers.py
  </files>
  <action>
    **Step 1: Create timeout wrapper.**
    Create `src/knowledge_hub/extraction/timeout.py`:

    - `async def extract_with_timeout(url: str, timeout_seconds: float = 30.0) -> ExtractedContent`:
      - Wraps the entire extraction pipeline in `async with asyncio.timeout(timeout_seconds)`.
      - On `TimeoutError`, returns `ExtractedContent(url=url, content_type=detect_content_type(url), extraction_status=ExtractionStatus.FAILED, extraction_method="timeout")`.

    - Internal `async def _extract_pipeline(url: str) -> ExtractedContent`:
      - Calls `detect_content_type(url)` to determine content type.
      - Dispatches to the appropriate extractor:
        - `ContentType.VIDEO` -> `extract_youtube(url)`
        - `ContentType.PDF` -> `extract_pdf(url)`
        - `ContentType.ARTICLE` / `ContentType.NEWSLETTER` / anything else -> `extract_article(url)`
      - Handles retry logic: one retry on transient failures (network errors, 5xx-like exceptions), still within the 30s budget. Before retrying, check `time.monotonic()` against the deadline -- if less than 3 seconds remain, skip retry and return whatever partial result exists.
      - Uses `time.monotonic()` at entry to compute a deadline for retry budget checking.

    - Transient failure detection: catch `httpx.HTTPError`, `ConnectionError`, `OSError` as retryable. Let `TranscriptsDisabled`, `NoTranscriptFound`, `VideoUnavailable`, `InvalidVideoId` pass through without retry (they are permanent failures handled by the extractors themselves).

    **Step 2: Create public API.**
    Update `src/knowledge_hub/extraction/__init__.py`:
    - Change from stub docstring to actual re-exports.
    - Import and re-export: `extract_content` (alias for `extract_with_timeout`), `detect_content_type`, `ExtractionStatus`.
    - The public API is: `from knowledge_hub.extraction import extract_content`.

    **Step 3: Wire into Slack handler.**
    Update `src/knowledge_hub/slack/handlers.py`:
    - In `process_message_urls()`, after creating each `SlackEvent`, call `await extract_content(url)` from `knowledge_hub.extraction`.
    - Log the extraction result status.
    - This is the Phase 3 handoff point -- Phase 4 (LLM processing) will consume the `ExtractedContent` result. For now, just log it.
    - Add the import: `from knowledge_hub.extraction import extract_content`.
  </action>
  <verify>
    - `uv run python -c "from knowledge_hub.extraction import extract_content; print('pipeline import OK')"` succeeds
    - `uv run python -c "from knowledge_hub.extraction.timeout import extract_with_timeout; print('timeout import OK')"` succeeds
    - `uv run pytest tests/ -v` -- all existing tests still pass
  </verify>
  <done>
    extract_with_timeout() wraps the full extraction pipeline in a 30-second asyncio.timeout. Retry logic attempts one retry on transient failures while respecting the remaining time budget. extract_content() is the public API re-exported from knowledge_hub.extraction. Slack handler calls extract_content() for each URL in the background task.
  </done>
</task>

<task type="auto">
  <name>Task 2: Comprehensive TDD tests for all extraction components</name>
  <files>
    tests/test_extraction/__init__.py
    tests/test_extraction/test_router.py
    tests/test_extraction/test_article.py
    tests/test_extraction/test_youtube.py
    tests/test_extraction/test_pdf.py
    tests/test_extraction/test_paywall.py
    tests/test_extraction/test_pipeline.py
  </files>
  <action>
    Create `tests/test_extraction/__init__.py` (empty).

    **TDD approach:** Write tests first for each component, run to confirm RED, then verify they pass against the implementation from Plan 01 + Task 1.

    **test_router.py** -- Content type routing tests (sync, no async needed):
    - `test_youtube_watch_url` -- `youtube.com/watch?v=xxx` -> VIDEO
    - `test_youtube_short_url` -- `youtu.be/xxx` -> VIDEO
    - `test_youtube_shorts_url` -- `youtube.com/shorts/xxx` -> VIDEO
    - `test_youtube_embed_url` -- `youtube.com/embed/xxx` -> VIDEO
    - `test_youtube_with_extra_params` -- `youtube.com/watch?v=xxx&t=120&list=PLxxx` -> VIDEO
    - `test_pdf_url` -- `example.com/doc.pdf` -> PDF
    - `test_pdf_url_with_query` -- `example.com/doc.pdf?dl=1` -> PDF
    - `test_pdf_url_case_insensitive` -- `example.com/DOC.PDF` -> PDF
    - `test_substack_url` -- `newsletter.substack.com/p/hello` -> NEWSLETTER
    - `test_medium_url` -- `medium.com/@user/article` -> ARTICLE (Medium uses article extractor)
    - `test_medium_custom_domain` -- `blog.medium.com/article` -> ARTICLE
    - `test_unknown_url` -- `example.com/page` -> ARTICLE (default)
    - `test_bare_domain` -- `https://nytimes.com` -> ARTICLE

    **test_article.py** -- Article extractor tests (async, mock trafilatura):
    - `test_extract_article_success` -- Mock `fetch_url` returning HTML, mock `bare_extraction` returning Document-like object with text, title, author, date, sitename. Verify ExtractedContent has correct field mapping and `extraction_status=FULL`.
    - `test_extract_article_fetch_fails` -- Mock `fetch_url` returning `None`. Verify `extraction_status=FAILED`.
    - `test_extract_article_extraction_fails` -- Mock `fetch_url` OK, mock `bare_extraction` returning `None`. Verify `extraction_status=METADATA_ONLY` or `FAILED`.
    - `test_extract_article_word_count` -- Verify word_count is calculated from text.
    - `test_extract_article_paywalled_domain` -- Mock URL from nytimes.com with short/empty text. Verify `extraction_status=PARTIAL`.
    - Use `unittest.mock.patch` to mock `trafilatura.fetch_url` and `trafilatura.bare_extraction`. Mock `asyncio.to_thread` if needed, or patch the trafilatura functions directly (they're called inside to_thread).

    **test_youtube.py** -- YouTube extractor tests (async, mock youtube-transcript-api):
    - `test_extract_video_id_watch` -- standard watch URL
    - `test_extract_video_id_short` -- youtu.be URL
    - `test_extract_video_id_shorts` -- shorts URL
    - `test_extract_video_id_embed` -- embed URL
    - `test_extract_video_id_with_params` -- URL with timestamp and list params
    - `test_extract_video_id_invalid` -- non-YouTube URL returns None
    - `test_extract_youtube_success` -- Mock `ytt_api.fetch()` returning transcript snippets. Verify ExtractedContent has joined transcript text, `extraction_status=FULL`, `content_type=VIDEO`.
    - `test_extract_youtube_transcripts_disabled` -- Mock raising `TranscriptsDisabled`. Verify `extraction_status=METADATA_ONLY`.
    - `test_extract_youtube_no_transcript` -- Mock raising `NoTranscriptFound`. Verify `extraction_status=METADATA_ONLY`.
    - `test_extract_youtube_video_unavailable` -- Mock raising `VideoUnavailable`. Verify `extraction_status=FAILED`.
    - `test_extract_youtube_invalid_id` -- Call with non-YouTube URL. Verify `extraction_status=FAILED`.
    - Mock the `YouTubeTranscriptApi` class and its `fetch` method using `unittest.mock.patch`.

    **test_pdf.py** -- PDF extractor tests (async, mock httpx + pypdf):
    - `test_extract_pdf_success` -- Mock httpx HEAD + GET returning PDF bytes, mock PdfReader with pages that have text. Verify ExtractedContent has text, `extraction_status=FULL`.
    - `test_extract_pdf_too_large_head` -- Mock HEAD returning Content-Length > 20MB. Verify `extraction_status=METADATA_ONLY`, no GET request made.
    - `test_extract_pdf_too_large_body` -- Mock HEAD with no Content-Length, GET returns >20MB body. Verify `extraction_status=METADATA_ONLY`.
    - `test_extract_pdf_no_text` -- Mock PDF with pages that return empty text (scanned/image PDF). Verify `extraction_status=METADATA_ONLY`.
    - `test_extract_pdf_metadata` -- Mock PdfReader.metadata with title and author. Verify these map to ExtractedContent fields.
    - `test_extract_pdf_download_error` -- Mock httpx raising HTTPError. Verify `extraction_status=FAILED`.
    - Mock `httpx.AsyncClient` and `pypdf.PdfReader`.

    **test_paywall.py** -- Paywall detection tests (sync):
    - `test_is_paywalled_known_domain` -- nytimes.com, wsj.com return True
    - `test_is_paywalled_with_www` -- www.nytimes.com returns True (subdomain handling)
    - `test_is_paywalled_unknown_domain` -- example.com returns False
    - `test_load_paywalled_domains` -- verify the YAML file loads and returns a set
    - `test_paywalled_domains_not_empty` -- verify config file has at least 5 domains

    **test_pipeline.py** -- Pipeline integration tests (async, mock extractors):
    - `test_pipeline_routes_youtube` -- Mock extract_youtube, call extract_content with YouTube URL, verify extract_youtube was called.
    - `test_pipeline_routes_pdf` -- Mock extract_pdf, call extract_content with PDF URL, verify extract_pdf was called.
    - `test_pipeline_routes_article` -- Mock extract_article, call extract_content with generic URL, verify extract_article was called.
    - `test_pipeline_timeout` -- Mock an extractor that sleeps >30s. Verify extract_content returns within ~30s with `extraction_status=FAILED`.
    - `test_pipeline_retry_on_transient_error` -- Mock extractor that fails once then succeeds. Verify success result (retry worked).
    - `test_pipeline_no_retry_on_permanent_error` -- Mock extractor raising `TranscriptsDisabled`. Verify no retry, returns result directly.
    - Use `unittest.mock.patch` on the extractor functions within the timeout module.

    Run `uv run pytest tests/ -v` to verify all tests pass.
  </action>
  <verify>
    - `uv run pytest tests/test_extraction/ -v` -- all new tests pass
    - `uv run pytest tests/ -v` -- all tests pass (no regressions)
    - `uv run pytest tests/ -v --tb=short | tail -5` shows total test count (should be ~60+ tests: ~23 existing + ~35+ new)
  </verify>
  <done>
    Comprehensive test suite covers all extraction components: router (13 cases), article extractor (5 cases), YouTube extractor (11 cases), PDF extractor (6 cases), paywall detection (5 cases), pipeline integration (6 cases). All tests use mocked external dependencies. All tests pass. No regressions in existing test suite.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -v` -- all tests pass (existing + new)
2. `uv run python -c "from knowledge_hub.extraction import extract_content"` -- public API importable
3. New test count is 35+ covering all extraction components
4. No test makes real HTTP calls or requires network access
5. Pipeline timeout test confirms 30-second budget enforcement
</verification>

<success_criteria>
- extract_content() is a single async function that accepts a URL and returns ExtractedContent
- 30-second wall-clock timeout wraps the entire extraction per URL
- One retry on transient failures respects remaining time budget
- Tests cover happy path, failure paths, and edge cases for every extractor
- All tests mock external dependencies (trafilatura, youtube-transcript-api, pypdf, httpx)
- Full test suite (existing + new) passes without regressions
</success_criteria>

<output>
After completion, create `.planning/phases/03-content-extraction/03-02-SUMMARY.md`
</output>
