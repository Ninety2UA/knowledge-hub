---
phase: 04-llm-processing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/knowledge_hub/llm/schemas.py
  - src/knowledge_hub/llm/client.py
  - src/knowledge_hub/llm/prompts.py
  - pyproject.toml
  - uv.lock
autonomous: true
requirements:
  - LLM-01
  - LLM-02
  - LLM-03
  - LLM-04
  - LLM-07
  - LLM-08
  - LLM-09
  - LLM-10

must_haves:
  truths:
    - "LLMResponse Pydantic model validates structured JSON with all required fields (title, summary, category, priority, tags, 4 body sections)"
    - "Gemini client initializes with API key from settings and provides async interface"
    - "System prompt includes all 11 categories, priority criteria, seeded tag list, and output format instructions"
    - "Content-type-specific prompt variants exist for videos, short content, and standard articles"
    - "Seeded tag list covers all 11 categories plus cross-cutting themes"
  artifacts:
    - path: "src/knowledge_hub/llm/schemas.py"
      provides: "LLMResponse and LLMKeyLearning Pydantic models for Gemini structured output"
      contains: "class LLMResponse"
    - path: "src/knowledge_hub/llm/client.py"
      provides: "Gemini client singleton with async support"
      contains: "get_gemini_client"
    - path: "src/knowledge_hub/llm/prompts.py"
      provides: "System prompt templates and content builder functions"
      contains: "build_system_prompt"
    - path: "pyproject.toml"
      provides: "google-genai and tenacity dependencies"
      contains: "google-genai"
  key_links:
    - from: "src/knowledge_hub/llm/schemas.py"
      to: "knowledge_hub.models.knowledge"
      via: "imports Category, Priority enums"
      pattern: "from knowledge_hub\\.models\\.knowledge import Category, Priority"
    - from: "src/knowledge_hub/llm/client.py"
      to: "knowledge_hub.config"
      via: "reads gemini_api_key from settings"
      pattern: "get_settings.*gemini_api_key"
    - from: "src/knowledge_hub/llm/prompts.py"
      to: "knowledge_hub.models.content"
      via: "uses ContentType and ExtractedContent for prompt routing"
      pattern: "from knowledge_hub\\.models\\.content import"
---

<objective>
Create the LLM foundation modules: response schema, Gemini client, and prompt templates.

Purpose: These three modules are the building blocks for Phase 4's processor. The schema defines what Gemini must return, the client handles API connectivity, and the prompts encode the knowledge curation instructions. All three are independent of each other and must be built before the processor can wire them together.

Output: Three production modules (`schemas.py`, `client.py`, `prompts.py`) plus `google-genai` and `tenacity` dependencies installed.
</objective>

<execution_context>
@/Users/dbenger/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dbenger/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-llm-processing/04-RESEARCH.md

@src/knowledge_hub/models/knowledge.py
@src/knowledge_hub/models/notion.py
@src/knowledge_hub/models/content.py
@src/knowledge_hub/config.py
@src/knowledge_hub/llm/__init__.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies and create LLM response schema</name>
  <files>pyproject.toml, uv.lock, src/knowledge_hub/llm/schemas.py</files>
  <action>
1. Add `google-genai>=1.64.0` and `tenacity>=9.1.4` to `pyproject.toml` dependencies. Run `uv sync` to install and update lockfile.

2. Create `src/knowledge_hub/llm/schemas.py` with two Pydantic models:

**LLMKeyLearning(BaseModel):**
- `what: str` -- the insight or learning
- `why_it_matters: str` -- why a practitioner should care
- `how_to_apply: list[str] = Field(min_length=1)` -- concrete, sequential steps

**LLMResponse(BaseModel):**
- `title: str = Field(description="Concise, descriptive title for the knowledge entry")`
- `summary: str = Field(description="3-5 sentence executive summary")`
- `category: Category = Field(description="Best-fit category from the 11 options")` -- import from `knowledge_hub.models.knowledge`
- `priority: Priority = Field(description="High/Medium/Low based on actionability")` -- import from `knowledge_hub.models.knowledge`
- `tags: list[str] = Field(min_length=3, max_length=7, description="3-7 relevant tags")`
- `summary_section: str = Field(description="3-5 sentence summary for the page body")`
- `key_points: list[str] = Field(min_length=5, max_length=10, description="5-10 importance-ordered key points")`
- `key_learnings: list[LLMKeyLearning] = Field(min_length=3, max_length=7)`
- `detailed_notes: str = Field(description="Structured breakdown, ~1500-2500 words")`

Key design: LLMResponse contains ONLY fields the LLM generates. Fields like `source`, `date_added`, `status`, `content_type` come from extraction metadata in the processor (Plan 02). This model is used as `response_schema` for Gemini structured output.

Note: Category and Priority are `str` enums -- Gemini's structured output enforces enum constraint, only valid values accepted.
  </action>
  <verify>
Run `uv sync` (should succeed). Run `python -c "from knowledge_hub.llm.schemas import LLMResponse, LLMKeyLearning; print('OK')"` to verify imports work.
  </verify>
  <done>google-genai and tenacity installed in lockfile. LLMResponse model importable with all required fields and Field constraints. Category and Priority enums reused from existing models.</done>
</task>

<task type="auto">
  <name>Task 2: Create Gemini client singleton and prompt templates</name>
  <files>src/knowledge_hub/llm/client.py, src/knowledge_hub/llm/prompts.py</files>
  <action>
1. Create `src/knowledge_hub/llm/client.py`:

**get_gemini_client() -> genai.Client:**
- Module-level `_client: genai.Client | None = None` for singleton caching
- Reads `gemini_api_key` from `get_settings()`
- Creates `genai.Client(api_key=..., http_options=types.HttpOptions(timeout=60_000))` -- 60s HTTP timeout in milliseconds
- Import: `from google import genai` and `from google.genai import types`
- Do NOT configure `HttpRetryOptions` -- tenacity handles retries at the application level (per research recommendation to avoid double-retry)

Also add a `reset_client()` function that sets `_client = None` -- needed for testing.

2. Create `src/knowledge_hub/llm/prompts.py`:

**SEEDED_TAGS constant** (module-level list):
Store the full seeded tag list from research (~55 tags covering 11 categories + cross-cutting themes). Tags are lowercase, hyphenated. Include:
- Category-derived: ai, machine-learning, deep-learning, llms, prompt-engineering, marketing, content-marketing, seo, paid-acquisition, email-marketing, product-management, product-strategy, user-research, roadmapping, growth, growth-loops, retention, activation, onboarding, analytics, data-science, experimentation, metrics, dashboards, engineering, architecture, devops, api-design, performance, design, ux, ui, design-systems, accessibility, business, strategy, fundraising, pricing, marketplace, career, leadership, management, hiring, mentoring, productivity, automation, workflows, tools, note-taking
- Cross-cutting: tutorial, case-study, research, frameworks, best-practices, startup, enterprise, open-source, trends

**GEMINI_MODEL constant:** `"gemini-3-flash-preview"` -- stored as constant, not config, per research guidance.

**_BASE_SYSTEM_PROMPT** (module-level string):
System prompt that instructs Gemini to act as a knowledge base curator. Must include:
- Title requirements (concise, descriptive)
- Summary requirements (3-5 dense sentences)
- Category instructions (list all 11 values from Category enum)
- Priority criteria (High = actionable/novel, Medium = useful reference, Low = tangential/thin)
- Tag instructions with `{seeded_tags}` placeholder for `.format()` substitution
- summary_section requirements (3-5 sentences, optimized for page body context)
- key_points requirements (5-10, importance-ordered NOT source-appearance-ordered -- LLM-08)
- key_learnings requirements (3-7 blocks, What/Why/How structure -- LLM-07)
- detailed_notes requirements (~1500-2500 words, markdown headers/bullets)
- Tone directive: professional, concise, actionable

**_VIDEO_ADDENDUM** (string):
Additional instructions for video content: include timestamp references, note duration context, focus on transcript not visual descriptions.

**_SHORT_CONTENT_ADDENDUM** (string):
Instructions for content under 500 words: proportionally shorter output, skip detailed_notes (return empty string), reduce key_points to 3-5, reduce key_learnings to 2-3.

**build_system_prompt(content: ExtractedContent) -> str:**
- Start with `_BASE_SYSTEM_PROMPT.format(seeded_tags=", ".join(SEEDED_TAGS))`
- If `content.content_type == ContentType.VIDEO`: append `_VIDEO_ADDENDUM`
- If `content.content_type` in `(ContentType.THREAD, ContentType.LINKEDIN_POST)` and `(content.word_count or 0) < 500`: append `_SHORT_CONTENT_ADDENDUM`
- Also apply short content addendum for ANY content type with word_count < 500 (not just threads/LinkedIn)
- Return the assembled prompt string

**build_user_content(content: ExtractedContent) -> str:**
- Build user message from extracted content metadata + body
- Include title, author, source_domain as labeled lines if present
- Use `content.transcript` for videos (primary body), `content.text` for articles, `content.description` as fallback
- Separate metadata from body with `\n---\n`
- Return assembled string

Key design decisions:
- Seeded tags are a module constant (not config) so they can be updated without editing prompt text -- per anti-patterns in research
- Model name is a constant, not config -- per research recommendation for preview model management
- Prompt uses `.format()` not f-string so the tag list is injected at call time, not module load time
  </action>
  <verify>
Run `python -c "from knowledge_hub.llm.client import get_gemini_client, reset_client; print('client OK')"` and `python -c "from knowledge_hub.llm.prompts import build_system_prompt, build_user_content, SEEDED_TAGS, GEMINI_MODEL; print(f'prompts OK, {len(SEEDED_TAGS)} tags, model={GEMINI_MODEL}')"` to verify imports.
  </verify>
  <done>Gemini client singleton initializes from settings with 60s timeout. Prompt templates cover base instructions, video addendum, and short content addendum. Seeded tags stored as module constant. Content-type routing logic dispatches correct prompt variant. build_user_content assembles metadata + body from ExtractedContent.</done>
</task>

</tasks>

<verification>
1. `uv sync` completes without errors
2. `python -c "from knowledge_hub.llm.schemas import LLMResponse; from knowledge_hub.llm.client import get_gemini_client; from knowledge_hub.llm.prompts import build_system_prompt, build_user_content; print('All imports OK')"` succeeds
3. All existing tests still pass: `uv run pytest tests/ -q`
4. LLMResponse model can be instantiated with valid data and rejects invalid data (e.g., missing required fields, category not in enum)
</verification>

<success_criteria>
- google-genai and tenacity added to pyproject.toml and installed in lockfile
- LLMResponse schema defined with all required fields, Field constraints, and correct Category/Priority enum imports
- Gemini client singleton reads API key from config and provides cached genai.Client
- System prompt covers all output requirements (LLM-02, LLM-03, LLM-07, LLM-08, LLM-09)
- Content-type-specific prompt variants work for videos and short content (LLM-10)
- Seeded tag list has ~55 tags covering all categories (LLM-04)
- Existing test suite unaffected
</success_criteria>

<output>
After completion, create `.planning/phases/04-llm-processing/04-01-SUMMARY.md`
</output>
