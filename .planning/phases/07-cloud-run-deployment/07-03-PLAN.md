---
phase: 07-cloud-run-deployment
plan: 03
type: execute
wave: 2
depends_on:
  - "07-01"
  - "07-02"
files_modified:
  - src/knowledge_hub/digest.py
  - src/knowledge_hub/config.py
  - src/knowledge_hub/app.py
  - tests/test_digest.py
  - tests/test_app.py
autonomous: true
requirements:
  - DEPLOY-06

must_haves:
  truths:
    - "POST /digest queries Notion for past week's entries and sends a Slack DM with formatted summary"
    - "POST /cost-check queries structured logs or accumulation for daily Gemini spend and sends Slack DM alert if over $5"
    - "Digest is always sent, even when zero entries (confirms service is running)"
    - "Logging is initialized at app startup via configure_logging()"
    - "Digest DM includes: entry count, titles with Notion links, category breakdown, top tags, total Gemini cost"
  artifacts:
    - path: "src/knowledge_hub/digest.py"
      provides: "Weekly digest builder and daily cost alert logic"
      contains: "build_weekly_digest"
    - path: "src/knowledge_hub/app.py"
      provides: "FastAPI app with /digest and /cost-check endpoints, logging initialization"
      contains: "digest"
    - path: "tests/test_digest.py"
      provides: "Tests for digest building and cost alert"
      contains: "test_build_weekly_digest"
  key_links:
    - from: "src/knowledge_hub/app.py"
      to: "src/knowledge_hub/logging_config.py"
      via: "configure_logging() called in lifespan"
      pattern: "configure_logging"
    - from: "src/knowledge_hub/app.py"
      to: "src/knowledge_hub/digest.py"
      via: "/digest and /cost-check route handlers"
      pattern: "digest"
    - from: "src/knowledge_hub/digest.py"
      to: "Notion API"
      via: "Query entries by Date Added for past week"
      pattern: "data_sources.query"
    - from: "src/knowledge_hub/digest.py"
      to: "Slack API"
      via: "chat_postMessage DM to allowed_user_id"
      pattern: "chat_postMessage"
---

<objective>
Implement the weekly digest endpoint, daily cost alert check, and wire logging initialization into the FastAPI app. The digest queries Notion for the previous week's entries and sends a formatted Slack DM. The cost check accumulates daily Gemini spend and alerts if over $5.

Purpose: Meet DEPLOY-06 (weekly digest) and complete the app.py wiring for structured logging (from Plan 01) and all new endpoints.
Output: digest.py module, updated app.py with logging + endpoints, updated config.py, tests.
</objective>

<execution_context>
@/Users/dbenger/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dbenger/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-cloud-run-deployment/07-RESEARCH.md
@.planning/phases/07-cloud-run-deployment/07-01-SUMMARY.md
@.planning/phases/07-cloud-run-deployment/07-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create digest module and wire app.py</name>
  <files>src/knowledge_hub/digest.py, src/knowledge_hub/config.py, src/knowledge_hub/app.py</files>
  <action>
1. Update `src/knowledge_hub/config.py`:
   - Add `scheduler_secret: str = ""` field to Settings. This is a shared secret that Cloud Scheduler sends in the request body/header to authenticate scheduled endpoint calls. Simple pragmatic approach per research recommendation.

2. Create `src/knowledge_hub/digest.py` with:

   **Imports:** datetime, timedelta, timezone, logging, collections.Counter, knowledge_hub.config.get_settings, knowledge_hub.notion.client (get_notion_client, get_data_source_id), knowledge_hub.slack.client.get_slack_client

   **`async def query_recent_entries(days: int = 7) -> list[dict]`:**
   - Calculate cutoff date: `(datetime.now(timezone.utc) - timedelta(days=days)).strftime("%Y-%m-%d")`
   - Get Notion client and data_source_id
   - Query with filter: `{"property": "Date Added", "date": {"on_or_after": cutoff}}`
   - Handle pagination: loop while `has_more` is True, pass `start_cursor` to next query
   - Return flat list of all page results

   **`def build_weekly_digest(entries: list[dict]) -> str`:**
   - If no entries: return "No entries processed this week. Service is running."
   - Otherwise build a formatted message:
     - Header: "*Weekly Knowledge Base Digest*\n_{start_date} to {end_date}_\n"
     - Entry count: "*{N} entries processed*\n"
     - Entry list: each entry as "- <{url}|{title}>" (extract from page properties -- Title is `title` type, Source is `url` type)
     - Category breakdown: count entries by Category property, format as "3 articles, 2 videos" etc.
     - Top tags: collect all tags from entries, count occurrences, show top 5
     - Total Gemini cost placeholder: this will be passed in as a parameter
   - Accept `total_cost: float = 0.0` parameter
   - Include at bottom: "*Total Gemini cost:* ${total_cost:.4f}"
   - Return the complete message string

   **Helper: `def _extract_entry_data(page: dict) -> dict`:**
   - Extract title from page properties (title type -> plain_text)
   - Extract source URL from properties (url type)
   - Extract category from properties (select type -> name)
   - Extract tags from properties (multi_select -> list of names)
   - Return dict with keys: title, url, category, tags

   **`async def send_weekly_digest() -> dict`:**
   - Call `query_recent_entries(days=7)`
   - Extract entry data from each page
   - Build digest message with `build_weekly_digest`
   - Note: total Gemini cost for the week will be estimated from entry count * average cost. Since structured logs contain per-entry costs but we cannot query Cloud Logging from within the app without adding a dependency, use a simpler approach: accept total_cost as parameter, or default to 0.0 with a note. The /digest endpoint handler can attempt a simple estimation if needed.
   - Actually, simplest approach per research: the cost.py module already logs cost per entry. For the digest, we can scan the entries and note "Cost tracking available in Cloud Run logs." OR we can add a simple in-memory cost accumulator to cost.py that the digest reads. Since --min-instances=1 keeps the instance alive, this is pragmatic.
   - Add to cost.py (or import from there): a module-level `_weekly_cost_accumulator: float = 0.0` and `_daily_cost_accumulator: float = 0.0` with functions `add_cost(amount: float)`, `get_weekly_cost() -> float`, `reset_weekly_cost()`, `get_daily_cost() -> float`, `reset_daily_cost()`. These are in-memory -- instance restart resets them, which is acceptable for a personal tool.
   - Import `get_weekly_cost, reset_weekly_cost` from cost.py
   - Get Slack client, send DM to `settings.allowed_user_id` with the digest message
   - Return `{"status": "sent", "entries": len(entries)}`

   **`async def check_daily_cost() -> dict`:**
   - Import `get_daily_cost, reset_daily_cost` from cost.py
   - If `get_daily_cost() > 5.0`:
     - Send Slack DM to allowed_user_id: "Daily Gemini cost alert: ${cost:.2f} exceeds $5.00 threshold"
     - Return `{"status": "alert_sent", "cost": cost}`
   - Else:
     - Return `{"status": "ok", "cost": cost}`
   - Note: do NOT reset daily cost here. The reset happens at midnight conceptually, but since Cloud Scheduler triggers this at 23:55, the daily cost will naturally reset when a new day's entries start logging.

   **Update cost.py** (add to the existing module from Plan 02):
   - Add module-level accumulators:
     ```python
     _daily_cost: float = 0.0
     _weekly_cost: float = 0.0
     ```
   - `add_cost(amount: float)`: adds to both `_daily_cost` and `_weekly_cost`
   - `get_daily_cost() -> float`: returns `_daily_cost`
   - `get_weekly_cost() -> float`: returns `_weekly_cost`
   - `reset_daily_cost()`: sets `_daily_cost = 0.0`
   - `reset_weekly_cost()`: sets `_weekly_cost = 0.0`
   - Update `log_usage()` to also call `add_cost(usage.cost_usd)` so costs accumulate automatically

3. Update `src/knowledge_hub/app.py`:
   - Import `configure_logging` from `knowledge_hub.logging_config`
   - Call `configure_logging()` at the top of the `lifespan` function (before yield, before settings load)
   - Import `send_weekly_digest`, `check_daily_cost` from `knowledge_hub.digest`
   - Import `get_settings` (already imported)
   - Add a `verify_scheduler(request: Request) -> None` dependency:
     - Read `X-Scheduler-Secret` header from request
     - Compare to `settings.scheduler_secret`
     - If mismatch or empty, raise HTTPException(403, "Invalid scheduler secret")
     - This protects /digest and /cost-check from unauthorized calls
   - Add `POST /digest` endpoint:
     - Depends on `verify_scheduler`
     - Calls `await send_weekly_digest()`
     - Returns the result dict
   - Add `POST /cost-check` endpoint:
     - Depends on `verify_scheduler`
     - Calls `await check_daily_cost()`
     - Returns the result dict
   - Update deploy.sh (if needed) to add `SCHEDULER_SECRET` to the `--set-secrets` mapping

  </action>
  <verify>
`python -c "from knowledge_hub.digest import send_weekly_digest, check_daily_cost, build_weekly_digest"` imports without error. `python -c "from knowledge_hub.app import app; [r.path for r in app.routes]"` shows /health, /slack/events, /digest, /cost-check.
  </verify>
  <done>
digest.py exists with query_recent_entries, build_weekly_digest, send_weekly_digest, check_daily_cost. config.py has scheduler_secret. app.py has logging init in lifespan, /digest and /cost-check endpoints with scheduler auth. cost.py has in-memory accumulators wired into log_usage.
  </done>
</task>

<task type="auto">
  <name>Task 2: Tests for digest module and app endpoints</name>
  <files>tests/test_digest.py, tests/test_app.py</files>
  <action>
1. Create `tests/test_digest.py` with:

   **test_build_weekly_digest_with_entries:**
   - Create mock entry data (list of dicts with title, url, category, tags)
   - Call `build_weekly_digest(entries, total_cost=0.003)`
   - Assert message contains: "Weekly Knowledge Base Digest", entry titles, category counts, top tags, "$0.0030"

   **test_build_weekly_digest_zero_entries:**
   - Call `build_weekly_digest([], total_cost=0.0)`
   - Assert message contains "No entries" and "Service is running"

   **test_extract_entry_data:**
   - Create a mock Notion page dict with properties in Notion API format (title, url, select, multi_select)
   - Call `_extract_entry_data(page)`
   - Assert returns dict with correct title, url, category, tags

   **test_query_recent_entries:**
   - Patch Notion client and data_source_id
   - Mock `client.databases.query` to return a page of results (with `has_more=False`)
   - Call `query_recent_entries(days=7)`
   - Assert returns the expected entries list

   **test_query_recent_entries_pagination:**
   - Mock `databases.query` to return `has_more=True` first, then `has_more=False`
   - Assert both pages of entries are returned

   **test_send_weekly_digest:**
   - Patch query_recent_entries, build_weekly_digest, get_slack_client, get_settings
   - Call `send_weekly_digest()`
   - Assert Slack `chat_postMessage` was called with `channel=settings.allowed_user_id`
   - Assert returns `{"status": "sent", "entries": N}`

   **test_check_daily_cost_under_threshold:**
   - Patch `get_daily_cost` to return 2.0
   - Call `check_daily_cost()`
   - Assert returns `{"status": "ok", "cost": 2.0}`
   - Assert Slack DM was NOT sent

   **test_check_daily_cost_over_threshold:**
   - Patch `get_daily_cost` to return 7.0
   - Patch get_slack_client
   - Call `check_daily_cost()`
   - Assert returns `{"status": "alert_sent", "cost": 7.0}`
   - Assert Slack DM was sent with alert text

2. Update `tests/test_app.py`:
   - Add `test_digest_endpoint_requires_auth`: POST /digest without scheduler secret returns 403
   - Add `test_cost_check_endpoint_requires_auth`: POST /cost-check without scheduler secret returns 403
   - Add `test_digest_endpoint_with_valid_auth`: POST /digest with correct X-Scheduler-Secret header. Patch send_weekly_digest to return mock result. Assert 200 response.
   - Add `test_cost_check_endpoint_with_valid_auth`: POST /cost-check with correct header. Patch check_daily_cost. Assert 200 response.

Run full test suite: `uv run pytest tests/ -x`
  </action>
  <verify>
`uv run pytest tests/test_digest.py -v` -- all digest tests pass. `uv run pytest tests/test_app.py -v` -- all app tests pass including endpoint auth tests. `uv run pytest tests/ -x` -- full suite passes.
  </verify>
  <done>
test_digest.py has 8+ tests covering digest building, entry extraction, Notion querying, digest sending, and cost alert logic. test_app.py has endpoint auth and success tests. Full test suite passes.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_digest.py -v` -- all digest tests pass
2. `uv run pytest tests/test_app.py -v` -- all app endpoint tests pass
3. `uv run pytest tests/ -x` -- full test suite green
4. `python -c "from knowledge_hub.app import app; print([r.path for r in app.routes])"` shows /digest and /cost-check
5. `grep "configure_logging" src/knowledge_hub/app.py` -- logging init present in lifespan
6. `grep "scheduler_secret" src/knowledge_hub/config.py` -- scheduler auth config present
7. `grep "allowed_user_id" src/knowledge_hub/digest.py` -- DM sent to correct user
</verification>

<success_criteria>
- POST /digest queries Notion for past week and sends formatted Slack DM with all required fields
- POST /cost-check checks daily Gemini spend and alerts if over $5
- Both endpoints are protected by scheduler secret authentication
- Zero-entry digest still sends ("Service is running" confirmation)
- Logging is initialized at app startup
- Full test suite passes
</success_criteria>

<output>
After completion, create `.planning/phases/07-cloud-run-deployment/07-03-SUMMARY.md`
</output>
