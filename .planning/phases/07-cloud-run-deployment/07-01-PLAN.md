---
phase: 07-cloud-run-deployment
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/knowledge_hub/logging_config.py
  - pyproject.toml
  - uv.lock
  - Dockerfile
  - deploy.sh
autonomous: true
requirements:
  - DEPLOY-01
  - DEPLOY-02
  - DEPLOY-03
  - DEPLOY-04
  - DEPLOY-05

must_haves:
  truths:
    - "Application emits structured JSON logs with GCP severity field to stdout"
    - "Deploy script configures Cloud Run with secrets, min-instances=1, and no-cpu-throttling"
    - "Slack signature verification is active (already implemented, just needs signing secret in Secret Manager)"
  artifacts:
    - path: "src/knowledge_hub/logging_config.py"
      provides: "Structured JSON logging configuration with GCP severity mapping"
      contains: "configure_logging"
    - path: "deploy.sh"
      provides: "Cloud Run deployment script with all gcloud flags"
      contains: "--set-secrets"
  key_links:
    - from: "src/knowledge_hub/logging_config.py"
      to: "python-json-logger"
      via: "pythonjsonlogger.json.JsonFormatter in dictConfig"
      pattern: "pythonjsonlogger"
    - from: "deploy.sh"
      to: "Google Secret Manager"
      via: "--set-secrets flag mounting env vars"
      pattern: "--set-secrets"
---

<objective>
Create the production infrastructure: structured JSON logging for Cloud Run, dependency updates, and a deployment script that configures secrets, CPU allocation, and cold start prevention.

Purpose: Establish the foundation modules and deployment tooling that all other Phase 7 work builds upon.
Output: logging_config.py module, updated pyproject.toml with python-json-logger, deploy.sh script with all Cloud Run flags.
</objective>

<execution_context>
@/Users/dbenger/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dbenger/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-cloud-run-deployment/07-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create structured JSON logging module</name>
  <files>src/knowledge_hub/logging_config.py, pyproject.toml, uv.lock</files>
  <action>
1. Add `python-json-logger>=3.2.1` to pyproject.toml dependencies and run `uv lock`.

2. Create `src/knowledge_hub/logging_config.py` with:
   - A `LOGGING_CONFIG` dict using `logging.config.dictConfig` format
   - Formatter: `pythonjsonlogger.json.JsonFormatter`
   - Format string: `%(asctime)s %(levelname)s %(name)s %(funcName)s %(message)s`
   - `rename_fields`: `{"levelname": "severity", "asctime": "timestamp", "name": "logger"}`
   - `static_fields`: `{"service": "knowledge-hub"}`
   - Handler: `logging.StreamHandler` writing to `ext://sys.stdout` (NOT stderr -- see research pitfall 4)
   - Root logger: level INFO, handler console
   - A `configure_logging()` function that calls `logging.config.dictConfig(LOGGING_CONFIG)`

This module is intentionally standalone. It will be wired into app.py lifespan in Plan 03 after digest endpoints are added.
  </action>
  <verify>
Run `uv lock` succeeds. Run `python -c "from knowledge_hub.logging_config import configure_logging; configure_logging(); import logging; logging.info('test')"` and verify JSON output with `severity`, `timestamp`, `logger`, `service` fields on stdout.
  </verify>
  <done>
logging_config.py exists with configure_logging(), python-json-logger is in pyproject.toml and uv.lock, JSON log output includes GCP-compatible `severity` field mapped from levelname.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create deploy script and verify Dockerfile</name>
  <files>deploy.sh, Dockerfile</files>
  <action>
1. Create `deploy.sh` as a bash script with the following:
   - Shebang: `#!/usr/bin/env bash`
   - `set -euo pipefail`
   - Variables at top: `PROJECT_ID`, `REGION=europe-west4`, `SERVICE_NAME=knowledge-hub`
   - A check that `PROJECT_ID` is set (exit with message if not)
   - Main `gcloud run deploy` command with:
     - `--source .` (source-based deploy, avoids needing Docker locally per research)
     - `--region europe-west4`
     - `--project=$PROJECT_ID`
     - `--set-secrets` mounting 6 secrets as env vars:
       - `SLACK_BOT_TOKEN=slack-bot-token:latest`
       - `SLACK_SIGNING_SECRET=slack-signing-secret:latest`
       - `NOTION_API_KEY=notion-api-key:latest`
       - `NOTION_DATABASE_ID=notion-database-id:latest`
       - `GEMINI_API_KEY=gemini-api-key:latest`
       - `ALLOWED_USER_ID=allowed-user-id:latest`
     - `--set-env-vars=ENVIRONMENT=production,LOG_LEVEL=INFO`
     - `--min-instances=1`
     - `--no-cpu-throttling` (NOT --cpu-always-allocated, which is deprecated per research)
     - `--cpu-boost`
     - `--memory=512Mi`
     - `--cpu=1`
     - `--allow-unauthenticated`
   - After deploy, print the service URL: `gcloud run services describe $SERVICE_NAME --region=$REGION --project=$PROJECT_ID --format='value(status.url)'`
   - A comment section at the bottom with Cloud Scheduler setup commands for digest and cost-check (these require the service URL, so documented but not auto-run):
     - Service account creation for scheduler
     - IAM policy binding for run.invoker
     - Weekly digest job: `0 8 * * 1` Europe/Amsterdam, POST /digest
     - Daily cost check job: `55 23 * * *` Europe/Amsterdam, POST /cost-check
   - `chmod +x deploy.sh`

2. Verify the existing Dockerfile is compatible with `--source .` deployment. The current Dockerfile uses `python:3.12-slim` + uv multi-layer caching which is Cloud Build compatible. No changes needed unless something is missing.

3. Ensure `.dockerignore` excludes `.planning/`, `tests/`, `.env`, and other non-production files. Read existing `.dockerignore` and add any missing exclusions.
  </action>
  <verify>
`bash -n deploy.sh` passes (syntax check). `chmod +x deploy.sh` succeeds. The deploy script contains all 6 secret mappings, `--min-instances=1`, `--no-cpu-throttling`, `--source .`, and `europe-west4`.
  </verify>
  <done>
deploy.sh exists and is executable, contains complete gcloud run deploy command with all secrets, CPU/memory config, and cold start prevention flags. Dockerfile is verified compatible. Cloud Scheduler setup commands are documented in the script.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from knowledge_hub.logging_config import configure_logging"` imports without error
2. `uv lock --check` passes (lockfile matches pyproject.toml)
3. `bash -n deploy.sh` passes syntax validation
4. `grep "severity" src/knowledge_hub/logging_config.py` finds the GCP severity mapping
5. `grep "min-instances" deploy.sh` finds cold start prevention flag
6. `grep "no-cpu-throttling" deploy.sh` finds CPU allocation flag
7. `grep "set-secrets" deploy.sh` finds all 6 secret mappings
</verification>

<success_criteria>
- Structured JSON logging module ready for import (not yet wired into app.py -- Plan 03)
- python-json-logger dependency added and locked
- Deploy script covers all 5 infrastructure requirements (DEPLOY-01 through DEPLOY-05)
- Cloud Scheduler setup documented in deploy script comments
- Existing Dockerfile verified compatible with source-based deploy
</success_criteria>

<output>
After completion, create `.planning/phases/07-cloud-run-deployment/07-01-SUMMARY.md`
</output>
